{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajrawat9075/CS6910_Assignment_1/blob/main/Final_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2ZBQJVGMrq"
      },
      "source": [
        "##Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lCl6FVyVxnq7"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gz01hfPzxZD-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMPvXuJ14mEi",
        "outputId": "6a920139-3a0c-43e6-8b29-6073c87f7628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kHsv_2bk-_A"
      },
      "source": [
        "## DATA Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjIRqdBhG4xX"
      },
      "source": [
        "###Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JPt8l_1gGxOn"
      },
      "outputs": [],
      "source": [
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the X_train into a training set and validation set\n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8JeMrFGHm0K"
      },
      "source": [
        "###Summarize loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaSGGKUnHlz1",
        "outputId": "da1e3c14-dfca-4908-c247-5f2bc88f8bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X = (54000, 28, 28), y = (54000,)\n",
            "Validation: X = (6000, 28, 28), y = (6000,)\n",
            "Test: X = (10000, 28, 28), y = (10000,)\n"
          ]
        }
      ],
      "source": [
        "print('Train: X = %s, y = %s' % (trainX.shape, trainY.shape))\n",
        "print('Validation: X = %s, y = %s' % (valX.shape, valY.shape))\n",
        "print('Test: X = %s, y = %s' % (testX.shape, testY.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT1jfyg3wdxO"
      },
      "source": [
        "### Display all class_names images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "FYZovbczwcKo",
        "outputId": "53a10131-6d22-4adc-fd08-bcc69a0323ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:qjwmf7pp) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training_Loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation_Loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training_Loss</td><td>2.34364</td></tr><tr><td>Training_accuracy</td><td>17.92037</td></tr><tr><td>Validation_Loss</td><td>2.34983</td></tr><tr><td>epoch</td><td>40</td></tr><tr><td>val_accuracy</td><td>17.18333</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\"> best parameter model</strong> at: <a href='https://wandb.ai/iitmadras/uncategorized/runs/qjwmf7pp' target=\"_blank\">https://wandb.ai/iitmadras/uncategorized/runs/qjwmf7pp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230320_084454-qjwmf7pp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:qjwmf7pp). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230320_085940-79x1uxbz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/iitmadras/dl_assignment_1/runs/79x1uxbz' target=\"_blank\">restful-water-619</a></strong> to <a href='https://wandb.ai/iitmadras/dl_assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/iitmadras/dl_assignment_1' target=\"_blank\">https://wandb.ai/iitmadras/dl_assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/iitmadras/dl_assignment_1/runs/79x1uxbz' target=\"_blank\">https://wandb.ai/iitmadras/dl_assignment_1/runs/79x1uxbz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_labels = [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"dl_assignment_1\"\n",
        ")\n",
        "# Get the unique labels\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "unique_labels = np.unique(trainY)\n",
        "print(\"unique_labels = %s\" % (unique_labels))\n",
        "\n",
        "# Create a subplot for each label\n",
        "image_list = []\n",
        "\n",
        "# Loop through the labels and display an image for each label\n",
        "for i, label in enumerate(unique_labels):\n",
        "    # Get the first image with this label\n",
        "    img = trainX[trainY == label][0]\n",
        "    # wandb.log({class_names[label] : axes[i].inshoe(img, cmap = 'gray')})\n",
        "    # Plot the image\n",
        "    image_list.append(wandb.Image(img, caption=class_names[label]))\n",
        "    \n",
        "\n",
        "wandb.log({\"Dataset\":image_list})   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_tQ6nolQAd"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eY0KnfCahr_Z"
      },
      "outputs": [],
      "source": [
        "# Normalize the pixel values to the range [0, 1]\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "valX = valX.astype('float32') / 255.0 \n",
        "\n",
        "trainSize = trainY.shape[0]\n",
        "testSize = testY.shape[0]\n",
        "\n",
        "#One-Hot encoding for trainY and testY and valY\n",
        "y_train = np.zeros(( 10, trainSize ))\n",
        "y_val = np.zeros(( 10, 6000 ))\n",
        "y_test = np.zeros(( 10, testSize ))\n",
        "\n",
        "for i in range(0, trainSize ):\n",
        "    y_train[trainY[i]][i] = 1\n",
        "\n",
        "for i in range(6000):\n",
        "    y_val[valY[i]][i] = 1\n",
        "\n",
        "for i in range(0, testSize ):\n",
        "    y_test[testY[i]][i] = 1\n",
        "\n",
        "trainY = y_train\n",
        "valY = y_val\n",
        "testY = y_test\n",
        "\n",
        "# reshape the X matrices\n",
        "trainX = trainX.reshape(trainX.shape[0], 784)\n",
        "valX = valX.reshape(6000, 784)\n",
        "testX = testX.reshape(10000, 784)\n",
        "\n",
        "trainX = trainX.T\n",
        "valX = valX.T\n",
        "testX = testX.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXjaOiY8k3PD"
      },
      "source": [
        "##Neural network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHguJ0NRWrby"
      },
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QCcJZxB6W_Op"
      },
      "outputs": [],
      "source": [
        "# sigmoid function that handles overflow\n",
        "def sigmoid(x):\n",
        "    x = 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
        "    return x\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    s = sigmoid(x)\n",
        "    return np.multiply(s, np.subtract(1, s))\n",
        "\n",
        "def ReLU( x):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function that avoids overflow.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def ReLU_deriv( x):\n",
        "    \"\"\"\n",
        "    Derivative of the ReLU activation function that avoids overflow.\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(x):\n",
        "    tanh_x = tanh(x)\n",
        "    return 1 - tanh_x**2\n",
        "\n",
        "def softmax(x):\n",
        "    x -= np.max(x, axis = 0)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis = 0)\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))\n",
        "\n",
        "def identity(x):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjBc4PgbaCue"
      },
      "source": [
        "### Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Cni40DPzXz9C"
      },
      "outputs": [],
      "source": [
        "def forward_prop(W, B, x, act_func = 'sigmoid'):\n",
        "\n",
        "    '''Function to forward propagate a minibatch of data once through the NN\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: numpy array\n",
        "        data in (features,batch_size) format\n",
        "\n",
        "    W: numpy array\n",
        "        contains all weights\n",
        "\n",
        "    B: numpy array\n",
        "        contains all biases\n",
        "\n",
        "    act_func: string\n",
        "        activation function to be used except the output layer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Y_: numpy array\n",
        "        contains the output probabilities for each class and each data sample after 1 pass\n",
        "    H: numpy array\n",
        "        contains all post-activations\n",
        "    A: numpy array\n",
        "        contsins all pre-activations\n",
        "\n",
        "    '''\n",
        "    L = W.shape[0]+1\n",
        "    A = [None] * (L - 1)\n",
        "\n",
        "    H = [None] * (L - 1)\n",
        "    Y_ = [None]\n",
        "\n",
        "    A[0] = B[0].reshape(-1, 1) + np.matmul(W[0], x) # for the first pre-activation layer x is the input \n",
        "\n",
        "    for i in range(0, L-2):\n",
        "        if act_func == \"sigmoid\":\n",
        "            H[i] = sigmoid(A[i])         # computing the activation layer \n",
        "\n",
        "        elif act_func == \"ReLU\":\n",
        "            H[i] = ReLU(A[i]) \n",
        "\n",
        "        elif act_func == \"tanh\":\n",
        "            H[i] = tanh(A[i]) \n",
        "\n",
        "        A[i+1] = B[i+1].reshape(-1, 1) + np.matmul(W[i+1], H[i])\n",
        "\n",
        "    Y_ = softmax(A[L-2])\n",
        "\n",
        "    return A, H, Y_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4XaXABaQZo"
      },
      "source": [
        "### Backward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "I4aWeEjRYNwN"
      },
      "outputs": [],
      "source": [
        "def back_prop(W, B,  x, y, A, H, Y_, act_func, loss_type):\n",
        "    '''Function to calculate gradients for a minibatch of data once through the NN through backpropagation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Y_: numpy array\n",
        "        output from forward propagation/ class probabilities\n",
        "\n",
        "    y: numpy array\n",
        "        actual class class_names in one hot\n",
        "     \n",
        "    H: numpy array\n",
        "        post-activations\n",
        "\n",
        "    A: numpy array\n",
        "        pre-activations   \n",
        "\n",
        "    W: numpy array\n",
        "        contains all weights\n",
        "\n",
        "    B: numpy array\n",
        "        contains all biases\n",
        "\n",
        "    act_func: string\n",
        "        activation function to be used except the output layer\n",
        "\n",
        "    batch_size: int\n",
        "        mini-batch-size\n",
        "\n",
        "    loss_type: string\n",
        "        loss function (MSE/Categorical crossentropy)\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    del_W: numpy array\n",
        "        gradients with respect to weights\n",
        "        \n",
        "    del_B: numpy array\n",
        "        gradients with respect to biases\n",
        "\n",
        "    '''\n",
        "    L = W.shape[0]+1\n",
        "\n",
        "    del_A,  del_H = A,  H  # creating gradient variables\n",
        "    del_W = [None] * (L-1)\n",
        "    del_B = [None] * (L-1)\n",
        "\n",
        "    if loss_type == \"cross_entropy\":\n",
        "        del_A[-1] = -(y - Y_)\n",
        "\n",
        "    elif loss_type == \"mean_squared_error\":\n",
        "        del_A[-1] = (2 / y.shape[1]) * softmax_derivative(Z[L-2])\n",
        "\n",
        "    for i in range(L-2, 0, -1):\n",
        "        del_W[i] = np.matmul(del_A[i],  np.transpose( H[i-1]))   # compute gradients with respect to weihts and bias\n",
        "        del_B[i] = del_A[i]\n",
        "        del_B[i] = np.array(np.sum(del_B[i], axis = 1))\n",
        "\n",
        "        del_H[i-1] = np.matmul(np.transpose(W[i]), del_A[i])\n",
        "\n",
        "        if act_func == \"sigmoid\":\n",
        "            del_A[i-1] = del_H[i-1] * sigmoid_deriv(A[i-1])\n",
        "        elif act_func == \"ReLU\":\n",
        "            del_A[i-1] = del_H[i-1] * ReLU_deriv(A[i-1])\n",
        "        elif act_func == \"tanh\":\n",
        "            del_A[i-1] = del_H[i-1] * tanh_deriv(A[i-1])  \n",
        "\n",
        "    del_W[0] = np.matmul(del_A[0] , np.transpose(x))  # compute gradients with respect to weihts and bias\n",
        "    del_B[0] = del_A[0]\n",
        "\n",
        "    del_B[0] = np.array(np.sum(del_B[0], axis = 1))\n",
        "\n",
        "\n",
        "    return del_W, del_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxYHiGMQabDZ"
      },
      "source": [
        "### Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6HRWIGyI8CDe"
      },
      "outputs": [],
      "source": [
        "def train(layers, X, Y, epochs, alpha, activation_func, optimizer, batch_size, weight_init, weight_decay, loss_type, momentum = 0.9, \n",
        "          beta = 0.9, beta1 = 0.9, beta2 = 0.999, eps = 0.00001):   \n",
        "    \"\"\"\n",
        "    parameters:\n",
        "\n",
        "    layers: list\n",
        "        contains all the layers no. of neurons\n",
        "\n",
        "    alpha : learning rate\n",
        "\n",
        "    return:\n",
        "\n",
        "    W: numpy array\n",
        "        contains all weights\n",
        "\n",
        "    B: numpy array\n",
        "        contains all biases\n",
        "\n",
        "    \"\"\"\n",
        "    L = len(layers) # no. of layers\n",
        "\n",
        "    # initialize the weights and biases\n",
        "    W = []\n",
        "    B = []\n",
        "    u_b, u_w, v_w, v_b = [], [], [], []\n",
        "    \n",
        "    # initialize the u_b, u_w, v_w, v_b\n",
        "\n",
        "    for i in range(1, L):\n",
        "        temp_w = np.zeros((layers[i], layers[i-1]))\n",
        "        temp_b = np.zeros((layers[i]))\n",
        "        u_w.append(temp_w)\n",
        "        v_w.append(temp_w)\n",
        "        u_b.append(temp_b)\n",
        "        v_b.append(temp_b)\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # initialize weights and biases\n",
        "    if weight_init == \"random\":\n",
        "        for i in range(1, L):\n",
        "            w = np.random.randn(layers[i], layers[i-1]) * 0.01\n",
        "            b = np.random.randn(layers[i]) * 0.01 # b is bias vector\n",
        "            W.append(w)\n",
        "            B.append(b)\n",
        "\n",
        "    if weight_init == \"Xavier\":\n",
        "        for i in range(1, L):\n",
        "            w = np.random.randn(layers[i], layers[i-1]) * np.sqrt(2.0 / (layers[i] + layers[i-1]))\n",
        "            b = np.random.randn(layers[i]) * np.sqrt(2.0 / (layers[i] + layers[i-1]))       # b is bias vector\n",
        "            W.append(w)\n",
        "            B.append(b)\n",
        "    \n",
        "    W = np.array(W)\n",
        "    B = np.array(B)\n",
        "    u_w = np.array(u_w)\n",
        "    v_w = np.array(v_w)\n",
        "    u_b = np.array(u_b)\n",
        "    v_b = np.array(v_b)\n",
        "    learning_rate = alpha\n",
        "    \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        alpha = learning_rate/(epoch+1)\n",
        "        for i in range(0, X.shape[1], batch_size):\n",
        "            batch_count = batch_size\n",
        "\n",
        "            if i + batch_size > X.shape[1]: # the last mini-batch might contain fewer than \"batch_size\" examples\n",
        "                batch_count = X.shape[1] - i + 1\n",
        "\n",
        "            if optimizer == 'sgd':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W, B, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W, B, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type)\n",
        "\n",
        "                W, B = update_parms_sgd(W, B, alpha, del_w, del_b, weight_decay)\n",
        "\n",
        "            elif optimizer == 'momentum':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W, B, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W, B, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type) # compute the gradient at the lookahead\n",
        "\n",
        "                u_w = momentum * u_w + del_w\n",
        "                u_b = momentum * u_b + del_b\n",
        "                W, B = update_parms_momentum(W, B, alpha, u_w, u_b,  weight_decay)\n",
        "\n",
        "            elif optimizer == 'nag':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W - beta * u_w, B - alpha * u_b, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W - beta * u_w, B - alpha * u_b, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type) # compute the gradient at the lookahead\n",
        "\n",
        "                u_w = momentum * u_w + del_w\n",
        "                u_b = momentum * u_b + del_b\n",
        "                \n",
        "                W, B = update_parms_nag(W, B, alpha, u_w, u_b,  weight_decay)\n",
        "\n",
        "            elif optimizer == 'rmsprop':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W, B, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W, B, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type) # compute the gradient at the lookahead\n",
        "\n",
        "                \n",
        "                W, B, u_w, u_b = update_parms_rmsprop(W, B, alpha, u_w, u_b, del_w, del_b, eps, beta,  weight_decay)\n",
        "\n",
        "            elif optimizer == 'adam':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W, B, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W, B, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type) # compute the gradient at the lookahead\n",
        "\n",
        "                \n",
        "                W, B, u_w, u_b, v_w, v_b = update_parms_adam(W, B, alpha, u_w, u_b, v_w, v_b, del_w, del_b, eps, beta1, beta2, epoch,  weight_decay)\n",
        "\n",
        "            elif optimizer == 'nadam':\n",
        "\n",
        "                A, H, Y_ = forward_prop(W, B, X[:, i:i+batch_count], activation_func)\n",
        "                del_w, del_b = back_prop(W, B, X[:, i:i+batch_count], Y[:, i:i+batch_count], A, H, Y_, activation_func, loss_type) # compute the gradient at the lookahead\n",
        "\n",
        "                W, B, u_w, u_b, v_w, v_b = update_parms_nadam(W, B, alpha, u_w, u_b, v_w, v_b, del_w, del_b, eps, beta1, beta2, epoch,  weight_decay)\n",
        "\n",
        "        show(W, B, activation_func, epoch, loss_type, weight_decay)\n",
        "\n",
        "    return W, B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### log data to wandb"
      ],
      "metadata": {
        "id": "0y4mFsHaPRGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "EcWZmXR7ieId"
      },
      "outputs": [],
      "source": [
        "def show(W, B, activ_f, epoch, loss_type, wd):\n",
        "    wandb.log({\"epoch\": epoch+1})\n",
        "\n",
        "    if(loss_type == \"cross_entropy\"):\n",
        "\n",
        "        # print the cross entropy validation loss\n",
        "        val_loss = loss(valX, valY, W, B, activ_f, wd)\n",
        "        wandb.log({\"Validation_Loss\": val_loss})\n",
        "\n",
        "        # print the cross entropy validation accuracy\n",
        "        val_accuracy = accuracy(valX, valY, W, B, activ_f)\n",
        "        wandb.log({\"val_accuracy\": val_accuracy})\n",
        "\n",
        "        # print the cross entropy training loss\n",
        "        training_loss = loss(trainX, trainY, W, B, activ_f, wd)\n",
        "        wandb.log({\"Training_Loss\": training_loss})\n",
        "\n",
        "        # print the cross entropy training accuracy\n",
        "        training_accuracy = accuracy(trainX, trainY,W, B, activ_f)\n",
        "        wandb.log({\"Training_accuracy\": training_accuracy})\n",
        "\n",
        "    elif(loss_type == \"mean_squared_error\"):\n",
        "\n",
        "        # print the mean_squared_error validation loss\n",
        "        val_loss = mean_squared_error(valX, valY, W, B, activ_f, wd)\n",
        "        wandb.log({\"Validation_Loss\": val_loss})\n",
        "\n",
        "        # print the mean_squared_error validation accuracy\n",
        "        val_accuracy = accuracy(valX, valY, W, B, activ_f)\n",
        "        wandb.log({\"val_accuracy\": val_accuracy})\n",
        "\n",
        "        # print the mean_squared_error training loss\n",
        "        training_loss = mean_squared_error(trainX, trainY, W, B, activ_f, wd)\n",
        "        wandb.log({\"Training_Loss\": training_loss})\n",
        "\n",
        "        # print the mean_squared_error training accuracy\n",
        "        training_accuracy = accuracy(trainX, trainY,W, B, activ_f)\n",
        "        wandb.log({\"Training_accuracy\": training_accuracy})\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A80aEK1LanJX"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TyVPxk9Oapf4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_parms_sgd(W, B, alpha, del_w, del_b, wd):\n",
        "    W = W - alpha * np.array(np.array(del_w)) - alpha * wd * W\n",
        "    B = B - alpha * np.array(del_b) - alpha * wd * B\n",
        "    return W, B\n",
        "\n",
        "def update_parms_nag(W, B, alpha, u_w, u_b, wd):\n",
        "    W = W - alpha * np.array(np.array(u_w)) - alpha * wd * W\n",
        "    B = B - alpha * np.array(u_b)- alpha * wd * B\n",
        "    return W, B\n",
        "\n",
        "def update_parms_momentum(W, B, alpha, u_w, u_b, wd):\n",
        "    W = W - alpha * np.array(np.array(u_w)) - alpha * wd * W\n",
        "    B = B - alpha * np.array(u_b) - alpha * wd * B\n",
        "    return W, B \n",
        "\n",
        "def update_parms_rmsprop(W, B, alpha, u_w, u_b, del_w, del_b, eps, beta, wd):\n",
        "\n",
        "    for i in range(W.shape[0]):\n",
        "        u_w[i] = beta * u_w[i] + (1-beta)*del_w[i]**2\n",
        "        u_b[i] = beta * u_b[i] + (1-beta)*del_b[i]**2\n",
        "        W[i] = W[i] - alpha * np.array(np.array(del_w[i])) / (np.sqrt(u_w[i]) + eps) - alpha * wd * W[i]\n",
        "        B[i] = B[i] - alpha * np.array(del_b[i]) / (np.sqrt(u_b[i]) + eps) - alpha * wd * B[i]\n",
        "    return W, B, u_w, u_b\n",
        "\n",
        "def update_parms_adam(W, B, alpha, u_w, u_b, v_w, v_b, del_w, del_b, eps, beta, beta2, epoch, wd):\n",
        "\n",
        "    for i in range(W.shape[0]):\n",
        "        v_w[i] = beta * v_w[i] + (1-beta) * del_w[i]\n",
        "        v_b[i] = beta * v_b[i] + (1-beta) * del_b[i]\n",
        "\n",
        "        v_w_hat = v_w[i] / (1 - beta ** (epoch+1))\n",
        "        v_b_hat = v_b[i] / (1 - beta ** (epoch+1))\n",
        "\n",
        "        u_w[i] = beta2 * u_w[i] + (1-beta2)*del_w[i]**2\n",
        "        u_b[i] = beta2 * u_b[i] + (1-beta2)*del_b[i]**2\n",
        "\n",
        "        u_w_hat = u_w[i] / (1- beta2 ** (epoch+1))\n",
        "        u_b_hat = u_b[i] / (1- beta2 ** (epoch+1))\n",
        "\n",
        "        W[i] = W[i] - alpha * np.array(np.array(v_w_hat)) / (np.sqrt(u_w_hat) + eps) - alpha * wd * W[i]\n",
        "        B[i] = B[i] - alpha * np.array(v_b_hat) / (np.sqrt(u_b_hat) + eps) - alpha * wd * B[i]\n",
        "\n",
        "    return W, B, u_w, u_b, v_w, v_b\n",
        "\n",
        "def update_parms_nadam(W, B, alpha, u_w, u_b, v_w, v_b, del_w, del_b, eps, beta, beta2, epoch, wd):\n",
        "\n",
        "    for i in range(W.shape[0]):\n",
        "        v_w[i] = beta * v_w[i] + (1-beta) * del_w[i]\n",
        "        v_b[i] = beta * v_b[i] + (1-beta) * del_b[i]\n",
        "\n",
        "        v_w_hat = v_w[i] / (1 - beta ** (epoch+1))\n",
        "        v_b_hat = v_b[i] / (1 - beta ** (epoch+1))\n",
        "\n",
        "        u_w[i] = beta2 * u_w[i] + (1-beta2)*del_w[i]**2\n",
        "        u_b[i] = beta2 * u_b[i] + (1-beta2)*del_b[i]**2\n",
        "\n",
        "        u_w_hat = u_w[i] / (1- beta2 ** (epoch+1))\n",
        "        u_b_hat = u_b[i] / (1- beta2 ** (epoch+1))\n",
        "\n",
        "        W[i] = W[i] - (alpha / np.sqrt(u_w_hat + eps)) * (beta * v_w_hat + (1-beta)*del_w[i]/(1-beta**(epoch+1))) - alpha * wd * W[i]\n",
        "        B[i] = B[i] - (alpha / np.sqrt(u_b_hat + eps)) * (beta * v_b_hat + (1-beta)*del_b[i]/(1-beta**(epoch+1))) - alpha * wd * B[i]\n",
        "\n",
        "    return W, B, u_w, u_b, v_w, v_b\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNgnKEda8Ue"
      },
      "source": [
        "### Predictions and Evaluations and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J_c2Ewklb6cT"
      },
      "outputs": [],
      "source": [
        "def predict(X, W, B, act_fun):\n",
        "    '''\n",
        "    forward propagate once and calculate class_names\n",
        "\n",
        "    '''\n",
        "    _, _, output = forward_prop(W, B, X, act_fun)\n",
        "    predictions = np.argmax(output, axis=0)\n",
        "    return predictions\n",
        "\n",
        "def accuracy(X, Y, W, B, activation_f):\n",
        "    test_predictions = predict(X, W, B, activation_f)\n",
        "    y_test = np.argmax(Y, axis=0)\n",
        "    return accuracy_score(y_test, test_predictions) * 100\n",
        "\n",
        "def loss(X, Y, W, B, activation_f, wd):\n",
        "\n",
        "    _, _, output = forward_prop(W, B, X, activation_f)\n",
        "    output = output.T\n",
        "    Y = Y.T\n",
        "    eps = 1e-12\n",
        "    loss = -np.mean(np.sum(Y * np.log(output + eps), axis=1))\n",
        "    return loss + 0.5 * wd ** 2\n",
        "\n",
        "# added the squared loss funtion\n",
        "def mean_squared_error(X, Y, W, B, activation_f, wd):\n",
        "\n",
        "    _, _, output = forward_prop(W, B, X, activation_f)\n",
        "    output = output.T\n",
        "    Y = Y.T\n",
        "    loss = np.mean(np.sum((Y - output) ** 2, axis = 1))\n",
        "    return loss + 0.5 * wd ** 2\n",
        "\n",
        "def evaluate(X_train, y_train, X_test, y_test, W, B, activation_f):\n",
        "    '''\n",
        "    print train,test accuracies and the classification report using sklearn\n",
        "\n",
        "    '''\n",
        "    y_train = np.argmax(y_train, axis=0)\n",
        "    train_predictions = predict(X_train, W, B, activation_f)\n",
        "    y_test = np.argmax(y_test, axis=0)\n",
        "    test_predictions = predict(X_test, W, B, activation_f)\n",
        "\n",
        "    print(\"Training accuracy = \", accuracy_score(y_train, train_predictions))\n",
        "    print(\"Test accuracy = \", accuracy_score(y_test, test_predictions))\n",
        "\n",
        "    return train_predictions, test_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifsc1vfGdyrM"
      },
      "source": [
        "## Hyperparameter tuning using Wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxNw7QeU5rDG"
      },
      "source": [
        "#### sweep config for wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh3puOAV5oqm"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'name' : 'first sweep',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'weight_init': {\n",
        "            'values': ['random', \"Xavier\"]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values' : [5, 10,20, 40]\n",
        "        },\n",
        "        \"hidden_layers\": {\n",
        "            \"values\": [ 3,4,5,6]\n",
        "        },\n",
        "        \"size_of_layer\": {\n",
        "            \"values\": [ 32, 64,128]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.001, 0.0001,0.00001]\n",
        "        },\n",
        "        \n",
        "        'batch_size': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nadam', 'sgd', 'rmsprop', 'nestrov', 'momentum']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['ReLU', 'sigmoid', 'tanh']\n",
        "        },\n",
        "        'weight_decay':{\n",
        "            'values': [0.0001, 0.0005, 0]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjbEgBcNmqXf"
      },
      "source": [
        "#### train_wand function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIRvb-bgDdI9"
      },
      "outputs": [],
      "source": [
        "def train_wand(config=None, loss_type = \"cross_entropy\"):\n",
        "    config_defaults = {\n",
        "        'epochs': 10,\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 1e-3,\n",
        "        'activation_f': 'ReLU',\n",
        "        'optimizer': 'adam',\n",
        "        'init_mode': 'xavier',\n",
        "        'L2_lamb': 0,\n",
        "        'num_neurons': 64,\n",
        "        'num_hidden': 3\n",
        "    }\n",
        "    wandb.init(config = config)\n",
        "    config = wandb.config\n",
        "    layers = [config.size_of_layer] * (config.hidden_layers+1)\n",
        "    layers[0] = 28*28\n",
        "    layers.append(10)\n",
        "    name='hl_'+str(config.hidden_layers)+\"_lr_\"+str(config.learning_rate)+\"_bs_\"+str(config.batch_size)+\"_opt_\"+str(config.optimizer)+ '_act_'+str(config.activation)\n",
        "    wandb.init(name = name)\n",
        "    W, B = train(layers, trainX, trainY, epochs=config.epochs, alpha = config.learning_rate, activation_func=config.activation, \n",
        "              optimizer =config.optimizer, batch_size=config.batch_size, weight_init = config.weight_init,weight_decay = config.weight_decay, loss_type )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BjD7I3vm9_q"
      },
      "source": [
        "#### Wand Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TvnfRb_A8YZy"
      },
      "outputs": [],
      "source": [
        "# Initialize WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config,project=\"dl_assignment_1\")\n",
        "wandb.agent(sweep_id=sweep_id,function=train_wand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PzRlJCy5zJX"
      },
      "source": [
        "## Testing the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dplQUXiZy1r6"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "size_of_layer = 128\n",
        "epochs = 40\n",
        "activation_func = 'tanh'\n",
        "optimizer = 'RMSprop'\n",
        "batch_size = 128\n",
        "weight_init = 'Xavier'\n",
        "weight_decay = 0\n",
        "hidden_layers = 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [size_of_layer] * (hidden_layers+1)\n",
        "layers[0] = 28*28\n",
        "layers.append(10)\n",
        "W, B = train(layers, trainX, trainY, epochs=epochs, alpha = learning_rate, activation_func=activation_func, \n",
        "              optimizer =optimizer, batch_size=batch_size, weight_init = weight_init,weight_decay = weight_decay, loss_type = \"cross_entropy\")\n"
      ],
      "metadata": {
        "id": "_bN6DWFR-w9a"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions, test_predictions = evaluate(trainX, trainY, testX, testY, W, B, activation_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GZRs4XtQ7NE",
        "outputId": "b768e636-1591-4d00-e900-66059a953833"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy =  0.8906666666666667\n",
            "Test accuracy =  0.8856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "dyz59E6fPDka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### confustion matrix with hovering"
      ],
      "metadata": {
        "id": "M3TClE1aNGoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "y_pred = test_predictions\n",
        "y_true = testY\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Create the hover text for the plot\n",
        "hovertext = []\n",
        "for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        text = f\"True class: {class_names[i]}<br>Predicted class: {class_names[j]}\"\n",
        "        if i == j:\n",
        "            text += f\"<br>Count: {cm[i][j]}\"\n",
        "        else:\n",
        "            text += f\"<br>Count: {cm[i][j]}<br>Predicted class for {class_names[i]}: {class_names[y_pred[(y_true == i) & (y_pred == j)][0]]}\"\n",
        "        hovertext.append(text)\n",
        "\n",
        "# Create the plot\n",
        "fig = go.Figure(data=go.Heatmap(z=cm, x=class_names, y=class_names, colorscale='Viridis', hovertext=hovertext))\n",
        "\n",
        "# Update the plot layout\n",
        "fig.update_layout(title='Confusion Matrix for Fashion MNIST', xaxis_title='Predicted Label', yaxis_title='True Label')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jRpjzJrPN_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###confustion matrix with accuracies"
      ],
      "metadata": {
        "id": "WP2r6idQNLeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.argmax(testY, axis = 0)\n",
        "cnf = confusion_matrix(y, test_predictions, normalize='true')\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.heatmap(cnf, annot=cnf,xtickclass_names=class_names, ytickclass_names=class_names)\n",
        "ax.set_title(\"Confusion Matrix\", size=16)\n",
        "ax.set_xlabel(\"Predicted Class\", size=14)\n",
        "ax.set_ylabel(\"True Class\", size=14)\n",
        "plt.savefig(\"confusion matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bWRylZ6NPCpP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlfFAw-yX8ui"
      },
      "source": [
        "## Training the model on mean square error loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the plots for accuracies and losses has been plotted in the report and also available in the wandb report"
      ],
      "metadata": {
        "id": "tftfa_bmSuoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [size_of_layer] * (hidden_layers+1)\n",
        "layers[0] = 28*28\n",
        "layers.append(10)\n",
        "W, B = train(layers, trainX, trainY, epochs=epochs, alpha = learning_rate, activation_func=activation_func, \n",
        "              optimizer =optimizer, batch_size=batch_size, weight_init = weight_init,weight_decay = weight_decay, loss_type = \"cross_entropy\")\n"
      ],
      "metadata": {
        "id": "B1iHomx5STVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing 3 models on MNSIT Dataset"
      ],
      "metadata": {
        "id": "29XLKSkCd0Vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create dataset"
      ],
      "metadata": {
        "id": "1kYYX2Q_e-1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\n",
        "# Split the X_train into a training set and validation set\n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "9Gik0nJVfCs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data processing"
      ],
      "metadata": {
        "id": "Qi3lU8mydz-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to the range [0, 1]\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "valX = valX.astype('float32') / 255.0 \n",
        "\n",
        "trainSize = trainY.shape[0]\n",
        "testSize = testY.shape[0]\n",
        "\n",
        "#One-Hot encoding for trainY and testY and valY\n",
        "y_train = np.zeros(( 10, trainSize ))\n",
        "y_val = np.zeros(( 10, 6000 ))\n",
        "y_test = np.zeros(( 10, testSize ))\n",
        "\n",
        "for i in range(0, trainSize ):\n",
        "    y_train[trainY[i]][i] = 1\n",
        "\n",
        "for i in range(6000):\n",
        "    y_val[valY[i]][i] = 1\n",
        "\n",
        "for i in range(0, testSize ):\n",
        "    y_test[testY[i]][i] = 1\n",
        "\n",
        "trainY = y_train\n",
        "valY = y_val\n",
        "testY = y_test\n",
        "\n",
        "# reshape the X matrices\n",
        "trainX = trainX.reshape(trainX.shape[0], 784)\n",
        "valX = valX.reshape(6000, 784)\n",
        "testX = testX.reshape(10000, 784)\n",
        "\n",
        "trainX = trainX.T\n",
        "valX = valX.T\n",
        "testX = testX.T"
      ],
      "metadata": {
        "id": "aQCVJsbOeQu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing for Hyperparameter Config 1"
      ],
      "metadata": {
        "id": "P0s5lxKAehx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "size_of_layer = 128\n",
        "epochs = 40\n",
        "activation_func = 'tanh'\n",
        "optimizer = 'RMSprop'\n",
        "batch_size = 128\n",
        "weight_init = 'Xavier'\n",
        "weight_decay = 0\n",
        "hidden_layers = 6"
      ],
      "metadata": {
        "id": "hfx1_yDCetEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [size_of_layer] * (hidden_layers+1)\n",
        "layers[0] = 28*28\n",
        "layers.append(10)\n",
        "W, B = train(layers, trainX, trainY, epochs=epochs, alpha = learning_rate, activation_func=activation_func, \n",
        "              optimizer =optimizer, batch_size=batch_size, weight_init = weight_init,weight_decay = weight_decay, loss_type = \"cross_entropy\")\n"
      ],
      "metadata": {
        "id": "QZlULNTRegWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions, test_predictions = evaluate(trainX, trainY, testX, testY, W, B, activation_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQbz6Ua0eg2J",
        "outputId": "b2efd462-7b6a-47e4-860b-ef03ae0ac5b3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy =  0.9992592592592593\n",
            "Test accuracy =  0.9762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config 2"
      ],
      "metadata": {
        "id": "AeC1poNIf3pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "size_of_layer = 128\n",
        "epochs = 20\n",
        "activation_func = 'tanh'\n",
        "optimizer = 'nadam'\n",
        "batch_size = 128\n",
        "weight_init = 'Xavier'\n",
        "weight_decay = 0.005\n",
        "hidden_layers = 6\n",
        "\n",
        "\n",
        "layers = [size_of_layer] * (hidden_layers+1)\n",
        "layers[0] = 28*28\n",
        "layers.append(10)\n",
        "W, B = train(layers, trainX, trainY, epochs=epochs, alpha = learning_rate, activation_func=activation_func, \n",
        "              optimizer =optimizer, batch_size=batch_size, weight_init = weight_init,weight_decay = weight_decay, loss_type = \"cross_entropy\")\n"
      ],
      "metadata": {
        "id": "IxWmzqMtf7-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions, test_predictions = evaluate(trainX, trainY, testX, testY, W, B, activation_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKSQdwff_ge",
        "outputId": "7be4d618-0cca-4774-ad68-11b8c261046a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy =  0.9938\n",
            "Test accuracy =  0.9799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### config 3"
      ],
      "metadata": {
        "id": "i7hRDSK-gHTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "size_of_layer = 64\n",
        "epochs = 10\n",
        "activation_func = 'tanh'\n",
        "optimizer = 'adam'\n",
        "batch_size = 128\n",
        "weight_init = 'Xavier'\n",
        "weight_decay = 0.005\n",
        "hidden_layers = 5\n",
        "\n",
        "\n",
        "layers = [size_of_layer] * (hidden_layers+1)\n",
        "layers[0] = 28*28\n",
        "layers.append(10)\n",
        "W, B = train(layers, trainX, trainY, epochs=epochs, alpha = learning_rate, activation_func=activation_func, \n",
        "              optimizer =optimizer, batch_size=batch_size, weight_init = weight_init,weight_decay = weight_decay, loss_type = \"cross_entropy\")\n"
      ],
      "metadata": {
        "id": "bLqPWZDfgKl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions, test_predictions = evaluate(trainX, trainY, testX, testY, W, B, activation_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2nKe3RngPfm",
        "outputId": "5f0bdf5c-ed6f-4bd0-fe58-544724abcff5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy =  0.9824\n",
            "Test accuracy =  0.9681\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxm0V/ZM7MEVO80fY7PuCP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}