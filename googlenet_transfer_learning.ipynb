{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\n#import tensorflow as tf\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nimport torch\nimport wandb\nwandb.login(key='e595ff5b95c353a42c4bd1f35b70856d4309ef00')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# data directory\ndata_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'\ndef get_transforms(data_augmentation):\n    if data_augmentation==\"yes\":\n        transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(degrees=30),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n            transforms.Resize((224,224)),\n            transforms.ToTensor()\n        ])\n    else:\n        transform = transforms.Compose([\n            transforms.Resize((224,224)),\n            transforms.ToTensor()\n        ])\n    return transform\n\ntransform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\ntrain_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=get_transforms(\"no\"))\ntest_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\nval_size = int(len(train_dataset) * 0.2)\ntrain_size = len(train_dataset) - val_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\ntrain_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\nval_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\ntest_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n#--------------------------------------------------------------------------------------------------------","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-12T15:20:42.707301Z","iopub.execute_input":"2023-04-12T15:20:42.707912Z","iopub.status.idle":"2023-04-12T15:20:44.387815Z","shell.execute_reply.started":"2023-04-12T15:20:42.707863Z","shell.execute_reply":"2023-04-12T15:20:44.386791Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets # added for googlenet\n#from torchvision.models import GoogLeNet\nimport torchvision.models as models\n\n\n# class implementation for googlenet \nclass MyGoogLeNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(MyGoogLeNet, self).__init__()\n        self.googlenet = models.googlenet(pretrained=True)\n        self.googlenet.fc = nn.Linear(self.googlenet.fc.in_features, num_classes)\n\n        # Freeze all layers except the last fully connected layer\n        for param in self.googlenet.parameters():\n            param.requires_grad = False\n        for param in self.googlenet.fc.parameters():\n            param.requires_grad = True\n            \n    #forward pass is implemented      \n    def forward(self, x):\n        x = self.googlenet(x)\n        return x\n\nsweep_config = {\n    'method': 'random', #grid, random,#bayes\n    'name' : 'Random_sweep_cross_entropy',\n    'metric': {\n      'name': 'valid accuracy',\n      'goal': 'maximize'  \n    },\n    'parameters': {\n        'optimizerr': {\n            'values': [\"sgd\"]\n        },\n        'learn_rate':{\n            'values':[\"0.001\"]\n        },\n         'momentumm':{\n            'values':[\".9\"]\n        },     \n        \n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='googlenet')\nfrom types import SimpleNamespace\ndef main():\n    with wandb.init() as run:\n        params={}\n        params=dict(wandb.config)\n        params=SimpleNamespace(**params)\n        run_name=\"goolenet training for inaturallist dataset\"\n        # Initialize the model\n        model = MyGoogLeNet().to(device)\n\n        # Define the loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n\n\n\n        # Train the model\n        num_epochs = 10\n        for epoch in range(num_epochs):\n            model.train()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                data, target = data.to(device), target.to(device)# Move data and target to the device\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n\n            # Evaluate the model on train_loader\n            model.eval()\n            train_correct = 0\n            train_total = 0\n            train_loss = 0.0\n            with torch.no_grad():\n                for data, target in train_loader:\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    loss = criterion(output, target)\n                    train_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    train_total += target.size(0)\n                    train_correct += (predicted == target).sum().item()\n\n            train_accuracy = 100 * train_correct/train_total\n            train_loss /= len(train_loader)\n\n            # Set model to evaluation mode\n            model.eval()\n            val_loss = 0.0\n            correct = 0\n            total = 0\n            with torch.no_grad():\n                for batch_idx, (data, target) in enumerate(val_loader):\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    loss = criterion(output, target)\n                    val_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    total += target.size(0)\n                    correct += (predicted == target).sum().item()\n\n            # Calculate validation metrics\n            val_loss /= len(val_loader)\n            val_accuracy = 100 * correct / total\n\n            # Print metrics for current epoch\n            #print('Epoch: {} \\t Training Loss: {:.6f}\n            print(\"epoch\",epoch)\n            print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n            print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n            wandb.log({'train loss':train_loss,'train accuracy':train_accuracy,'valid loss':val_loss,'valid accuracy':val_accuracy})\n\nwandb.agent(sweep_id, function=main,count=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:21:05.736228Z","iopub.execute_input":"2023-04-12T15:21:05.736659Z","iopub.status.idle":"2023-04-12T16:14:56.887578Z","shell.execute_reply.started":"2023-04-12T15:21:05.736625Z","shell.execute_reply":"2023-04-12T16:14:56.885443Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Create sweep with ID: voo0n295\nSweep URL: https://wandb.ai/cs22m010/googlenet/sweeps/voo0n295\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tk38x23u with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentumm: .9\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizerr: sgd\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230412_152109-tk38x23u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs22m010/googlenet/runs/tk38x23u' target=\"_blank\">peachy-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m010/googlenet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m010/googlenet/sweeps/voo0n295' target=\"_blank\">https://wandb.ai/cs22m010/googlenet/sweeps/voo0n295</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs22m010/googlenet' target=\"_blank\">https://wandb.ai/cs22m010/googlenet</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs22m010/googlenet/sweeps/voo0n295' target=\"_blank\">https://wandb.ai/cs22m010/googlenet/sweeps/voo0n295</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs22m010/googlenet/runs/tk38x23u' target=\"_blank\">https://wandb.ai/cs22m010/googlenet/runs/tk38x23u</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"epoch 0\nTraining Loss: 1.190815 \t Training Accuracy: 65.950000\nValidation Loss: 1.217925 \t Validation Accuracy: 65.082541\nepoch 1\nTraining Loss: 1.029540 \t Training Accuracy: 69.075000\nValidation Loss: 1.071627 \t Validation Accuracy: 66.333167\nepoch 2\nTraining Loss: 0.967437 \t Training Accuracy: 69.700000\nValidation Loss: 1.022048 \t Validation Accuracy: 67.033517\nepoch 3\nTraining Loss: 0.925018 \t Training Accuracy: 70.075000\nValidation Loss: 0.996571 \t Validation Accuracy: 68.234117\nepoch 4\nTraining Loss: 0.913335 \t Training Accuracy: 70.400000\nValidation Loss: 0.995556 \t Validation Accuracy: 67.683842\nepoch 5\nTraining Loss: 0.874065 \t Training Accuracy: 71.300000\nValidation Loss: 0.964023 \t Validation Accuracy: 69.384692\nepoch 6\nTraining Loss: 0.862811 \t Training Accuracy: 71.825000\nValidation Loss: 0.957113 \t Validation Accuracy: 68.734367\nepoch 7\nTraining Loss: 0.856424 \t Training Accuracy: 71.875000\nValidation Loss: 0.952825 \t Validation Accuracy: 69.184592\nepoch 8\nTraining Loss: 0.859292 \t Training Accuracy: 72.100000\nValidation Loss: 0.968031 \t Validation Accuracy: 68.884442\nepoch 9\nTraining Loss: 0.818514 \t Training Accuracy: 73.250000\nValidation Loss: 0.929988 \t Validation Accuracy: 69.934967\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train accuracy</td><td>▁▄▅▅▅▆▇▇▇█</td></tr><tr><td>train loss</td><td>█▅▄▃▃▂▂▂▂▁</td></tr><tr><td>valid accuracy</td><td>▁▃▄▆▅▇▆▇▆█</td></tr><tr><td>valid loss</td><td>█▄▃▃▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train accuracy</td><td>73.25</td></tr><tr><td>train loss</td><td>0.81851</td></tr><tr><td>valid accuracy</td><td>69.93497</td></tr><tr><td>valid loss</td><td>0.92999</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">peachy-sweep-1</strong> at: <a href='https://wandb.ai/cs22m010/googlenet/runs/tk38x23u' target=\"_blank\">https://wandb.ai/cs22m010/googlenet/runs/tk38x23u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230412_152109-tk38x23u/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}